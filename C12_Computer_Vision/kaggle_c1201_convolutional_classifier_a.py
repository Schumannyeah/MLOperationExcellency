# Welcome to Computer Vision!
# Have you ever wanted to teach a computer to see? In this course, that's exactly what you'll do!
#
# In this course, you'll:
#
# Use modern deep-learning networks to build an image classifier with Keras
# Design your own custom convnet with reusable blocks
# Learn the fundamental ideas behind visual feature extraction
# Master the art of transfer learning to boost your models
# Utilize data augmentation to extend your dataset

# At the end, you'll be ready to move on to more advanced applications like generative adversarial networks and image segmentation.

# The Convolutional Classifier
# A convnet used for image classification consists of two parts: a convolutional base and a dense head.
# The base is used to extract the features from an image. It is formed primarily of layers performing the convolution operation,
# but often includes other kinds of layers as well. (You'll learn about these in the next lesson.)
#
# The head is used to determine the class of the image. It is formed primarily of dense layers, but might include other layers like dropout.


# Training the Classifier
# The goal of the network during training is to learn two things:
#
# which features to extract from an image (base),
# which class goes with what features (head).

# These days, convnets are rarely trained from scratch. More often, we reuse the base of a pretrained model.
# To the pretrained base we then attach an untrained head. In other words, we reuse the part of a network that has already learned to do 1.
# Extract features, and attach to it some fresh layers to learn 2. Classify.

# Because the head usually consists of only a few dense layers, very accurate classifiers can be created from relatively little data.
#
# Reusing a pretrained model is a technique known as transfer learning.
# It is so effective, that almost every image classifier these days will make use of it.


# Imports
import os, warnings
import matplotlib.pyplot as plt
from matplotlib import gridspec

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow import keras

# Reproducability
# to set a seed for reproducibility
def set_seed(seed=31415):
    # This line sets the seed for NumPy's random number generator using np.random.seed(seed).
    # NumPy is a popular library for numerical computing in Python, and it includes its own random number generator.
    # By setting this seed, you ensure that any random numbers generated by NumPy will be reproducible.
    np.random.seed(seed)
    # for TensorFlow's random number generator using tf.random.set_seed(seed).
    tf.random.set_seed(seed)
    # Set an environment variable to the string representation of the seed
    os.environ['PYTHONHASHSEED'] = str(seed)
    # setting this variable tells TensorFlow to use deterministic operations where possible
    os.environ['TF_DETERMINISTIC_OPS'] = '1'

# This line calls the set_seed function with the seed value 31415.
# By calling this function, the seed is set for NumPy, TensorFlow, and environment variables,
# ensuring that any subsequent operations involving random numbers will be reproducible.
set_seed(31415)

# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
# sets the default colormap for images in matplotlib to "magma". This colormap will be used when displaying images using functions like plt.imshow().
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # to clean up output cells


# Load training and validation sets
ds_train_ = image_dataset_from_directory(
    # ../input/car-or-truck/train: This is the path to the directory containing the training images.
    # The directory is expected to have subdirectories, one for each class, with the images for that class inside. For example:
    '../input/car-or-truck/train',
    # labels='inferred': This tells the function to infer the labels from the directory structure.
    # The directory names (e.g., car, truck) are used as labels.
    labels='inferred',
    # label_mode='binary': This specifies that the labels are binary. Since there are two classes (cars and trucks),
    # the labels will be 0 or 1. If there were more than two classes, you would use label_mode='categorical'.
    label_mode='binary',
    # image_size=[128, 128]: This resizes all the images to 128x128 pixels.
    # This ensures that all images have the same dimensions, which is required for training a neural network.
    image_size=[128, 128],
    # interpolation='nearest': This specifies the interpolation method used when resizing images.
    # 'nearest' uses the nearest-neighbor interpolation. Other options include 'bilinear', 'bicubic', etc.
    interpolation='nearest',
    # batch_size=64: This sets the number of images in each batch.
    # During training, the dataset will be divided into batches of 64 images each.
    # Batching helps in efficient training and gradient computation.
    batch_size=64,
    # shuffle=True: This shuffles the dataset, which helps in mixing the data
    # so that the model does not learn any unintended patterns from the order of the data.
    # Shuffling is especially important for training to prevent the model from learning the order of the data.
    shuffle=True,
)
ds_valid_ = image_dataset_from_directory(
    '../input/car-or-truck/valid',
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],
    interpolation='nearest',
    batch_size=64,
    shuffle=False,
)

# Data Pipeline
# The function convert_to_float is designed to convert images to floating-point format and keep their associated labels unchanged.
# This is often necessary in machine learning and image processing tasks
def convert_to_float(image, label):
    # this function normalizes pixel values based on the original data type:
    # if the original data type is integer based like uint8, the pixel value are scaled to the range [0, 1]
    # if the original data type is already floating point, the pixel values might be adjusted to fit within the expected range
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

# AUTOTUNE is a constant provided by TensorFlow that enables automatic tuning of the buffer sizes for the dataset operations.
# When using AUTOTUNE, TensorFlow dynamically tunes the performance of data loading and preprocessing to improve training speed and efficiency.
AUTOTUNE = tf.data.experimental.AUTOTUNE
# map(convert_to_float): This applies the convert_to_float function to each element (image, label pair) in the dataset.
# cache(): This caches the dataset in memory after the first epoch.
# prefetch(buffer_size=AUTOTUNE): This overlaps the preprocessing and model execution of the data.
# While the model is training on one batch of data, the next batch can be prepared asynchronously.
ds_train = (
    ds_train_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
ds_valid = (
    ds_valid_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)

# # This will print the available endpoints. If serving_default is available, then the code you provided is correct.
# # If another endpoint is listed, replace 'serving_default' with the appropriate endpoint name.
# saved_model_path = '../input/cv-course-models/cv-course-models/vgg16-pretrained-base'
# print(tf.saved_model.load(saved_model_path).signatures)

# The most commonly used dataset for pretraining is ImageNet, a large dataset of many kind of natural images.
# Keras includes a variety models pretrained on ImageNet in its applications module. The pretrained model we'll use is called VGG16.



# =====================================================
# The error occurs because TFSMLayer does not support setting the trainable attribute directly.
# To make the layers non-trainable, you need to modify the model in a different way.
# Unfortunately, TFSMLayer does not allow setting its layers as non-trainable directly.
#
# Instead, you can load a pre-built VGG16 model from Keras applications with pre-trained ImageNet weights,
# which allows setting trainable to False.

# ----------------------------------------------------
# the following original codes don't work under keras 3

# # pretrained_base = tf.keras.models.load_model(
# #     '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
# # )
# pretrained_base = tf.keras.layers.TFSMLayer(
#     '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
#     call_endpoint='serving_default'
# )
# pretrained_base.trainable = False
# ----------------------------------------------------





# Load pretrained VGG16 model from Keras applications
pretrained_base = tf.keras.applications.VGG16(
    include_top=False,
    weights='imagenet',
    input_shape=(128, 128, 3)
)
pretrained_base.trainable = False



# Step 3 - Attach Head
# Next, we attach the classifier head. For this example, we'll use a layer of hidden units (the first Dense layer)
# followed by a layer to transform the outputs to a probability score for class 1, Truck.
# The Flatten layer transforms the two dimensional outputs of the base into the one dimensional inputs needed by the head.


from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # pretrained_base is a pre-existing model that is included as the first layer in the sequential model.
    # In this context, it seems like pretrained_base is likely a convolutional base,
    # such as VGG16 or another pre-trained model loaded earlier in your code.
    # This layer is used to extract features from input images.
    pretrained_base,
    # layers.Flatten() is a Keras layer that flattens the input, i.e., it converts a 2D matrix into a 1D vector.
    # This is typically used to prepare the output of the convolutional base (which is a 3D tensor) for input into a dense layer.
    layers.Flatten(),
    # layers.Dense(6, activation='relu') is a fully connected (dense) layer with 6 units and ReLU activation function.
    # Dense layers are fully connected layers where each neuron is connected to every neuron in the previous layer.
    # The number 6 denotes the dimensionality of the output space, and ReLU (Rectified Linear Unit) is the activation function,
    # which introduces non-linearity.
    layers.Dense(6, activation='relu'),
    # layers.Dense(1, activation='sigmoid') is another dense layer with 1 unit and sigmoid activation function.
    # In binary classification tasks like this one, sigmoid activation is often used in the output layer
    # to produce a probability score indicating the likelihood of the input belonging to the positive class.
    layers.Dense(1, activation='sigmoid'),
])


# Step 4 - Train
# Finally, let's train the model. Since this is a two-class problem, we'll use the binary versions of crossentropy and accuracy.
# The adam optimizer generally performs well, so we'll choose it as well.

# The model.compile() function in Keras is where you configure the learning process before training the model.
model.compile(
    # The optimizer parameter specifies the optimization algorithm used to update the weights of the neural network during training.
    # 'adam' is a popular optimization algorithm that stands for Adaptive Moment Estimation. 自适应矩估计
    optimizer='adam',
    # The loss parameter specifies the loss function used to evaluate
    # how well the model's predictions match the actual target values during training.
    # 'binary_crossentropy' is commonly used for binary classification tasks.
    # It measures the difference between probability distributions,
    # in this case, between predicted probabilities and true binary labels (0s and 1s).
    loss='binary_crossentropy',
    # 'binary_accuracy' is a metric that calculates how often predictions match binary labels (0s and 1s).
    # It is the accuracy metric tailored for binary classification tasks.
    metrics=['binary_accuracy'],
)

# The model.fit() function in Keras is used to train a neural network model on a given dataset.
history = model.fit(
    ds_train,
    # The validation_data parameter specifies the dataset (ds_valid) used to evaluate the model after each epoch of training.
    # This dataset is not used for training; instead, it helps monitor the model's performance on unseen data and detect overfitting.
    validation_data=ds_valid,
    # The epochs parameter specifies the number of complete passes through the training dataset (ds_train).
    # Each epoch consists of one full cycle of forward and backward propagation through the neural network,
    # where weights are adjusted based on the optimizer and loss function.
    epochs=30,
    # Setting verbose=0 means silent mode, where no output is printed during training (except for errors or warnings).
    verbose=0,
)

# When training a neural network, it's always a good idea to examine the loss and metric plots.
# The history object contains this information in a dictionary history.history.
# We can use Pandas to convert this dictionary to a dataframe and plot it with a built-in method.

import pandas as pd

# history.history is a dictionary returned by model.fit() containing metrics (like loss, accuracy) recorded
# during training on both the training and validation datasets (ds_train and ds_valid).
# pd.DataFrame() converts this dictionary into a pandas DataFrame called history_frame,
# where each key-value pair in history.history becomes a column in the DataFrame.
history_frame = pd.DataFrame(history.history)
# This line plots the training (loss) and validation (val_loss) losses across epochs.
# history_frame.loc[:, ['loss', 'val_loss']] selects all rows (:) and columns ['loss', 'val_loss']
# from the history_frame DataFrame. .plot() is a pandas function that generates a plot using matplotlib or
# another plotting backend (depending on pandas configuration).
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();

plt.show()




